# HTTP 发展历史

## HTTP1.0 和 HTTP1.1 的区别

1. **缓存处理**：`HTTP1.0`使用`If-Modified-Since`，`Expires`来做缓存的处理；`HTTP1.1`加入更多的缓存策略来做缓存处理如：`Etag`、`If-Unmodified-Since`、`If-Match`、`Cache-Control`等头部。

2. **带宽优化及网络连接的使用**：`HTTP1.0`中存在一些浪费带宽的现象：如客户端只想请求服务器端某个对象的一部分数据，而服务器会将整个数据发送过来，并且不支持断点续传功能。`HTTP1.1`中加入`range`头部，它允许只请求资源的某个部分，即返回码是`206（Partial Content）`。

3. **状态信息的增加**，`HTTP1.1`中新增了许多状态码，可以更好的处理信息的结果。

4. **Host 头部的处理**：在`HTTP1.0`中认为每个服务器都绑定唯一的`IP`，因此请求的消息的`URL`并没有传递主机名，但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（`Multi-homed Web Servers`），并且它们共享一个`IP`地址。`HTTP1.1`的请求消息和响应消息都应支持`Host`头域，且请求消息中如果没有`Host`头域会报告一个错误（`400 Bad Request`）。

5. **长连接**：`HTTP1.1`中支持长连接和请求的流水线处理：在一个`TCP`连接上可以传送多个`HTTP`请求和响应，减少了建立和关闭`TCP`连接的损耗和延迟；而`HTTP1.0`每个`HTTP`请求都会经历一个`TCP`连接的建立和关闭。

## SPDY：HTTP1.X 的优化

其为`Google`推出的一套方案，优化了`HTTP`的请求延迟，提高了安全性，并达到了以下作用：

1. **多路复用**：针对`HTTP`的高延迟，`SPDY`采用了多路复用，通过多个请求共享一个`TCP`连接的方式，通过`streamId`来互相区别，解决了`HTTP1.1`中流水线传输[头阻塞](../../HTTP/UDP,TCP协议/TCP/README.md#%e6%8f%90%e9%97%ae)的问题，降低了延迟并提高了带宽利用率。

2. **请求优先级**： 由于多路复用带来了新的问题——请求被阻塞，为了解决该问题，`SPDY`允许给每个请求设置一个优先级，这样优先级高的请求会优先得到响应。

3. **header 压缩**：`HTTP1.x`的`header`很多时候都存在重复多余的情况。使用合适的压缩算法可以减少包的大小和数量。

4. **数据压缩**:对传输的数据使用`gzip`进行压缩

5. **基于 HTTPS 的加密协议传输**：提升数据的安全性

6. [**服务器推送**](#%e6%9c%8d%e5%8a%a1%e5%99%a8%e6%8e%a8%e9%80%81)

`SPDY`位于`HTTP`之下，`TCP`和`SSL`之上，这样可以轻松兼容老版本的`HTTP`协议(将`HTTP1.x`的内容封装成一种新的`frame`格式)，同时可以使用已有的`SSL`功能。

实质上，`SPDY`就是想不影响`HTTP`语义的情况下，替换`HTTP`底层传输的协议来加快页面加载时间。

`SPDY`的解决办法就是设计了一个会话层协议--帧协议，解决多路复用，优先级等问题，然后在其上实现了`HTTP`的语义。

Reference:
[1. The Chromium Projects-SPDY](http://www.chromium.org/spdy)

## HTTP2.0：SPDY 升级版

`HTTP/2`构建在`Google SPDY`协议基础之上的

### HTTP2.0 与 SPDY 区别

HTTP2.0 跟 SPDY 很大部分有相似之处，但它们也存在如下区别：

1. HTTP2.0 支持明文传输；SPDY 使用的是 HTTPS 传输
2. HTTP2.0 采用[HPACK](http://http2.github.io/http2-spec/compression.html)头部压缩算法，而 SPDY 采用[DEFLATE](http://zh.wikipedia.org/wiki/DEFLATE)

### HTTP1.0 与 HTTP2.0 区别

1. [**报文传输格式**](#%e6%8a%a5%e6%96%87%e4%bc%a0%e8%be%93%e6%a0%bc%e5%bc%8f%e4%ba%8c%e8%bf%9b%e5%88%b6%e5%88%86%e5%b8%a7)：`HTTP1.X`的解析是基于文本的；`HTTP2.0`是基于二级制格式的传输。

2. **多路复用**：即共享连接，一个请求对应一个`streamId`，接收方可以根据`streamId`来归纳属于哪个服务器；针对每个域只使用一个多路复用的连接，而不是每个文件一个连接；

3. [**头部压缩**](#%e9%a6%96%e9%83%a8%e5%8e%8b%e7%bc%a9)：HTTP2.0 使用`encoder`来减少需要传输的`header`大小，通讯双方各自缓存一份`header fields`表，既避免了重复`header`的传输，又减小了需要传输的大小。

4. **服务器推送**

Reference:
[1. HTTP,HTTP2.0,SPDY,HTTPS 你应该知道的一些事](http://www.alloyteam.com/2016/07/httphttp2-0spdyhttps-reading-this-is-enough/)

### HTTP2.0 详情

#### 报文传输格式——二进制分帧

HTTP2.0 在应用层与传输层之间新增了一个二进制分帧层

![二进制分帧](./imgs/binary%20format.jpg)

在二进制分帧层中， HTTP/2 会将所有传输的信息分割为更小的消息和帧（`frame`）,并对它们采用二进制格式的编码 ，其中 HTTP1.x 的首部信息会被封装到 `HEADER frame`，而相应的 `Request Body` 则封装到 `DATA frame` 里面。

#### 首部压缩

HTTP/1.1 并不支持 HTTP 首部压缩，为此 SPDY 和 HTTP/2 应运而生， `SPDY` 使用的是通用的`DEFLATE` 算法，而 HTTP/2 则使用了专门为首部压缩而设计的 `HPACK` 算法。

HTTP2.0 具体采取以下措施：

-   在客户端与服务器端使用“首部表”来跟踪和存储之前发送的首部的键值对，对于相同的数据，不再通过请求和响应发送。
-   首部表在 HTTP/2 的连接存续期内始终存在，由客户端和服务器共同渐进地更新
-   每个新的首部键值对要么被追加到当前表的末尾，要么替换表中之前的值

![头部压缩](./imgs/header%20compress.jpg)
[参考](https://my.oschina.net/editorial-story/blog/3031721)

#### 多路复用

虽然`HTTP2.0`解决了管道化队头阻塞的问题，但是仍在`TCP`层面上存在阻塞。

`HTTP/1.1` 时代建立一个 `TCP` 连接，三个请求组成一个队列发出去，服务器接收到这个队列之后会依次响应，一旦前面的请求阻塞，后面的请求就会无法响应。

`HTTP/2` 是通过分帧并且给每个帧打上流的 `ID` 去避免依次响应的问题，对方接收到帧之后根据 `ID` 拼接出流，这样就可以做到乱序响应从而避免请求时的队首阻塞问题。但是 `TCP` 层面的队首阻塞是 `HTTP/2` 无法解决的（`HTTP` 只是应用层协议，`TCP` 是传输层协议），`TCP` 的阻塞问题是因为传输阶段可能会丢包，一旦丢包就会等待重新发包，阻塞后续传输，

Reference:
[1. http2 是如何解决 tcp 的队首阻塞的？](https://www.zhihu.com/question/65900752/answer/490183849)

#### 服务器推送

服务器可以向客户端推送所需要的资源，避免不必要的往返传输，举个例子：

```html
<img src="pic.01.com" data-original="pic.zio.com" />
```

有个上述标签存在服务器返回的 html 文件中，当该文件返回时，服务器会向客户端同时发送对应的 css、js 文件，而无需通知客户端。此时多个往返传输就变成了一个。

##### 存在的问题

**假如此时客户端恰好有一份服务器端自己推送的文件的缓存，那么该怎么办**？

因为服务器推送本身具有投机性，所以可能会出现自己推送的文件是客户端不需要的情况。

> 在这种情况下，HTTP2.0 允许客户端通过 RESET_STREAM 主动取消 Push，但主动取消就意味着浪费了数据往返传输的价值。

对此的解决方法是：客户端使用一个简洁的`Cache Digest`来告诉服务器，哪些东西已经缓存，服务器知道后，就不会在 push 这些东西。因为 `Cache Digest` 使用的是 `Golumb Compressed Sets`，浏览器客户端可以通过一个连接发送少于 1K 字节的 `Packets` 给服务端，通知哪些是已经在缓存中存在的。

### 是否应该从 HTTP1.X 升级为 HTTP2.0

如果你使用 SSL/TLS（以后简称 TLS），那么 HTTP/2 可以提升网站性能。如果你没有，那在使用 HTTP/2 之前要先支持 TLS。这时候，使用 TLS 的性能损耗大致可以被使用 HTTP/2 的性能提升抵销。

#### HTTP2.0 的五个缺点

1. 单连接开销比较大：HPACK 数据压缩算法会更新两端的查找表。这样可以让连接有状态，而破坏状态意味着要重建查找表，另外单连接占用内存较多。

2. 你可能不需要 SSL：如果你的数据不需要保护，或者已经使用 DRM 或其他编码进行保护了，那么 TLS 的安全性对你可能无所谓。

3. 需要抛弃针对 HTTP1.X 的优化：HTTP/1.x 优化在支持 HTTP/2 的浏览器中会影响性能，因此可能需要花时间把它们推倒重来。

4. 对下载大文件不利：如果你的应用主要提供大文件下载或者流媒体播放，那可能不想用 TLS，而且在只有一个流的情况下，多路复用也体现不出什么优势。

##### 找出为 HTTP1.X 优化的代码

针对于 HTTP1.X 你的服务器也需做过以下的优化：

1. 分域存储：为了实现并行请求文件，你可能把文件分散在不同域中，例如 CDN。但分域存储会影响 HTTP2.0 的性能，建议针对 HTTP2.0 做出优化，保证以下两点：

    - 让多个域名解析到同一个 IP
    - 确保证书包含通配符，以便所有分域名都可以使用，适当的多域证书当然也可以。

    > 有了这些保障，分域还会继续对 HTTP/1.x 有效，即域名仍然可以触发浏览器创建更多连接，但对 HTTP/2 则无效，因为这些域名会被看成同一个域，一个连接就可以访问所有域名了。

2. 雪碧图：雪碧图即把多张图拼接为一张，然后通过代码按序获取图片。在 HTTP2.0 通过一个连接并行下载的情况下，雪碧图的作用不大。

3. 拼接的代码文件：与使用雪碧图的原因类似，很多独立的文件也会被弄成一个，然后浏览器再从其中找到并运行需要的文件。

4. 插入行内的文件：CSS 代码、JavaScript 代码，甚至图片等被直接插到 HTML 文件中的内容。这样可以减少文件传输，代价是初始 HTML 文件较大。

> 后三种优化的手段都是将小文件塞进一个较大的文件中，已达到减少新建连接的初始化和握手的目的，这些操作对 TSL 而言非常耗时间。
>
> 第一种优化则是强行打开多个连接，目的是并行从不同域下载文件。

上述两种优化方式都需要投入大量时间、精力和资源用于实现、管理和运维。

[HTTP1.0 至 HTTP2.0 的迁移](https://www.w3ctech.com/topic/1563#tip7sharding)
[Nginx HTTP2.0 白皮书](https://www.nginx.com/wp-content/uploads/2015/09/NGINX_HTTP2_White_Paper_v4.pdf)
[HTTP/2 对现在的网页访问，有什么大的优化呢？体现在什么地方？](https://www.zhihu.com/question/24774343/answer/96586977)

## QUIC：HTTP3

`Google`在推出`SPDY`时就意识到这些问题，于是基于`UDP`协议创建了一个`QUIC`协议，也就是后来的`HTTP3`，它完美的解决了`TCP`队头阻塞的问题。

它具有如下的特点：

### 快速握手——零 RTT(往返时间)建立连接

`QUIC`按以下流程建立连接：

1. 客户端向服务器端发送 _Inchoate Client Hello_，用于请求连接
2. 服务器生成`g/p/a`，并根据`g/p/a`算出`A`，然后将`g/p/A`放入*Server Config*中，再发送`Rejection`给客户端
3. 客户端收到`g/p/A`，自己生成`b`，再根据`g/p/b`算出`B`，再根据`A/p/b`算出初始化密钥`K`，客户端根据`K`加密`HTTP`数据，连同`B`一起发送给服务器。
4. 服务器端收到`B`后，根据`a/p/B`算出同样的密钥，然后解密对于的`HTTP`数据。为了进一步的安全(前向安全性)，服务端会更新自己的随机数`a`和公钥，再生成密钥`S`，然后把公钥通过`Server Hello`发送给客户端。连同`Server Hello`还有`HTTP`返回的信息。
5. 客户端收到`Server Hello`后，改用公钥`S`进行加密通信。

> 这里使用的是 DH 密钥交换算法，DH 算法的核心就是服务端生成 a、g、p 3 个随机数，a 自己持有，g 和 p 要传输给客户端，而客户端会生成 b 这 1 个随机数，通过 DH 算法客户端和服务端可以算出同样的密钥。在这过程中 a 和 b 并不参与网络传输，安全性大大提高。因为 p 和 g 是大数，所以即使在网络中传输的 p、g、A、B 都被劫持，那么靠现在的计算机算力也没法破解密钥。

### 连接迁移

TCP 连接基于四元组（源 IP、源端口、目的 IP、目的端口），切换网络时至少会有一个因素发生变化，导致连接发生变化。当连接发生变化时，如果还使用原来的 `TCP` 连接，则会导致连接失败，就得等原来的连接超时后重新建立连接，所以我们有时候发现切换到一个新网络时，即使新网络状况良好，但内容还是需要加载很久。如果实现得好，当检测到网络变化时立刻建立新的 `TCP` 连接，即使这样，建立新的连接还是需要几百毫秒的时间。

`QUIC`的连接以一个`64`位随机数为标识，它称为`Connection ID`，通过验证它的变化就能验证连接是否改变。

### TCP 线头阻塞/多路复用

`HTTP1.1-HTTP2.0`都存在队头阻塞问题(Head of line blocking)。

`TCP` 是个面向连接的协议，即发送请求后需要收到 `ACK` 消息，以确认对方已接收到数据。如果每次请求都要在收到上次请求的 `ACK` 消息后再请求，那么效率无疑很低。后来 `HTTP/1.1` 提出了 `Pipelining` 技术，允许一个 `TCP` 连接同时发送多个请求，这样就大大提升了传输效率。

`HTTP/2` 的多路复用解决了上述的队头阻塞问题。不像 `HTTP/1.1` 中只有上一个请求的所有数据包被传输完毕下一个请求的数据包才可以被传输，`HTTP/2` 中每个请求都被拆分成多个 `Frame` 通过一条 `TCP` 连接同时被传输，这样即使一个请求被阻塞，也不会影响其他的请求。

`HTTP/2` 虽然可以解决“请求”这个粒度的阻塞，但 `HTTP/2` 的基础 `TCP` 协议本身却也存在着队头阻塞的问题。`HTTP/2` 的每个请求都会被拆分成多个 `Frame`，不同请求的 `Frame` 组合成 `Stream`，`Stream` 是 TCP 上的逻辑传输单元，这样 HTTP/2 就达到了一条连接同时发送多条请求的目标，这就是多路复用的原理。例如在一条 `TCP` 连接上同时发送 4 个 `Stream`，其中 `Stream1` 已正确送达，`Stream2` 中的第 3 个 `Frame` 丢失，`TCP` 处理数据时有严格的前后顺序，先发送的 `Frame` 要先被处理，这样就会要求发送方重新发送第 3 个 `Frame`，`Stream3` 和 `Stream4` 虽然已到达但却不能被处理，那么这时整条连接都被阻塞。

不仅如此，由于 `HTTP/2` 必须使用 `HTTPS`，而 `HTTPS` 使用的 `TLS` 协议也存在队头阻塞问题。`TLS` 基于 `Record` 组织数据，将一堆数据放在一起（即一个 `Record`）加密，加密完后又拆分成多个 `TCP` 包传输。一般每个 `Record 16K`，包含 12 个 `TCP` 包，这样如果 12 个 `TCP` 包中有任何一个包丢失，那么整个 `Record` 都无法解密。

队头阻塞会导致 `HTTP/2` 在更容易丢包的弱网络环境下比 `HTTP/1.1` 更慢！

`QUIC`通过以下两点来解决这个问题：

-   QUIC 的传输单元是 `Packet`，加密单元也是 `Packet`，整个加密、传输、解密都基于 `Packet`，这样就能避免 `TLS` 的队头阻塞问题；
-   QUIC 基于 `UDP`，`UDP` 的数据包在接收端没有处理顺序，即使中间丢失一个包，也不会阻塞整条连接，其他的资源会被正常处理。

### 拥塞控制

拥塞控制的目的是避免过多的数据一下子涌入网络，导致网络超出最大负荷。`QUIC` 的拥塞控制与 [`TCP` 类似](../../HTTP/UDP,TCP协议/TCP/README.md)，`QUIC` 重新实现了 `TCP` 协议的 `Cubic` 算法进行拥塞控制，并在此基础上做了不少改进，如下：

-   **热插拔**：`TCP` 中如果要修改拥塞控制策略，需要在系统层面进行操作；`QUIC` 修改拥塞控制策略只需要在应用层操作，并且 `QUIC` 会根据不同的网络环境、用户来动态选择拥塞控制算法。

-   **前向纠错**：`QUIC` 使用前向纠错(`FEC，Forward Error Correction`)技术增加协议的容错性。一段数据被切分为 10 个包后，依次对每个包进行异或运算，运算结果会作为 `FEC` 包与数据包一起被传输，如果不幸在传输过程中有一个数据包丢失，那么就可以根据剩余 9 个包以及 `FEC` 包推算出丢失的那个包的数据，这样就大大增加了协议的容错性。
-   **单调递增的 Packet Number**：`QUIC`的`Packet Number`严格单调递增，即使丢失了在重传时也会是一个新的递增数字，这样发送方接受的确认消息就能知道`ACK`对应的是原始请求还是重传请求。
-   **ACK Delay**：`TCP` 计算 `RTT` 时没有考虑接收方接收到数据到发送确认消息之间的延迟，如下图所示，这段延迟即 `ACK` `Delay`。`QUIC` 考虑了这段延迟，使得 `RTT` 的计算更加准确。
-   **更多的 ACK 块**：一般来说，接收方收到发送方的消息后都应该发送一个 `ACK` 回复，表示收到了数据。但每收到一个数据就返回一个 `ACK` 回复太麻烦，所以一般不会立即回复，而是接收到多个数据后再回复，`TCP SACK` 最多提供 3 个 `ACK block`。但有些场景下，比如下载，只需要服务器返回数据就好，但按照 `TCP` 的设计，每收到 3 个数据包就要“礼貌性”地返回一个 `ACK`。而 `QUIC` 最多可以捎带 256 个 `ACK block`。在丢包率比较严重的网络下，更多的`ACK block` 可以减少重传量，提升网络效率。

### 流量控制

这段不想写了，看下面文章吧。

Reference:

[1. 知乎-HTTP/3 原理实战](https://zhuanlan.zhihu.com/p/143464334)
